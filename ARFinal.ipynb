{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful commentary on defects4j: https://greg4cr.github.io/pdf/20d4j.pdf\n",
    "May need this to set path: export PATH=$PATH:/Users/clairecallon/defects4j/framework/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nfile_path = os.path.join(buggy_dir, \"src\", \"test\", \"java\", \"org\", \"apache\", \"commons\",\"math3\", \"geometry\", \"euclidean\",\"twod\", \"LineTest.java\")\\n\\n# Open and read the file\\nwith open(file_path, \\'r\\') as file:\\n    content = file.read()\\n    print(content)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "DEFECTS4J_HOME = \"/Users/clairecallon/defects4j\"  # Adjust to your path\n",
    "os.environ[\"DEFECTS4J_HOME\"] = DEFECTS4J_HOME\n",
    "os.environ[\"PATH\"] += os.pathsep + f\"{DEFECTS4J_HOME}/framework/bin\"\n",
    "\n",
    "def checkout_version(project, bug_id, version):\n",
    "    \"\"\"Checkout a specific version of a project from Defects4J.\"\"\"\n",
    "    work_dir = f\"/tmp/{project}_{bug_id}_{version}\"\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    \n",
    "    cmd = f\"defects4j checkout -p {project} -v {bug_id}{version} -w {work_dir}\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error:\", result.stderr)\n",
    "    \n",
    "    return work_dir\n",
    "\n",
    "# Checkout buggy and fixed versions\n",
    "buggy_dir = checkout_version(\"Math\", \"35\", \"b\")  # Buggy version\n",
    "fixed_dir = checkout_version(\"Math\", \"35\", \"f\")  # Fixed version\n",
    "src_dir = os.path.join(buggy_dir, \"src\")\n",
    "\n",
    "\n",
    "path = \"/tmp/Math_35_b/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\"\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "file_path = os.path.join(buggy_dir, \"src\", \"test\", \"java\", \"org\", \"apache\", \"commons\",\"math3\", \"geometry\", \"euclidean\",\"twod\", \"LineTest.java\")\n",
    "\n",
    "# Open and read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now begin training neural networks\n",
    "#first tokenize\n",
    "import javalang\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def tokenize_java_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Tokenize a single Java file using javalang.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        java_code = f.read()\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(java_code))\n",
    "        return [token.value for token in tokens]\n",
    "    except javalang.tokenizer.LexerError:\n",
    "        print(f\"Lexer error in file: {file_path} (possibly invalid Java syntax)\")\n",
    "        return []\n",
    "\n",
    "def tokenize_defects4j_project(project_dir: str) -> List[List[str]]:\n",
    "    \"\"\"Tokenize all Java files in a Defects4J project.\"\"\"\n",
    "    all_tokens = []\n",
    "    for root, _, files in os.walk(project_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".java\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                tokens = tokenize_java_file(file_path)\n",
    "                if tokens:  # Only add if tokenization succeeded\n",
    "                    all_tokens.append(tokens)\n",
    "    return all_tokens\n",
    "\n",
    "# Example: Tokenize a Defects4J buggy version\n",
    "project_dir = \"/tmp/Lang_1_b\"  # Path after checking out Lang-1\n",
    "buggy_tokens = tokenize_defects4j_project(project_dir)\n",
    "project_dir = \"/tmp/Lang_1_f\"\n",
    "fixed_tokens = tokenize_defects4j_project(project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Tuple\n",
    "import difflib\n",
    "\n",
    "# 1. Find differences between buggy and fixed files to create token-level labels\n",
    "def identify_bug_locations(buggy_tokens: List[str], fixed_tokens: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Identify which tokens in the buggy file are likely to contain bugs by comparing with fixed tokens.\n",
    "    Returns a list of 0s and 1s where 1 indicates a potentially buggy token.\n",
    "    \"\"\"\n",
    "    # Use difflib to find differences\n",
    "    matcher = difflib.SequenceMatcher(None, buggy_tokens, fixed_tokens)\n",
    "    bug_locations = [0] * len(buggy_tokens)\n",
    "    \n",
    "    # Mark tokens that are different or absent in the fixed version\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in ('replace', 'delete'):\n",
    "            for i in range(i1, i2):\n",
    "                if i < len(bug_locations):  # Safety check\n",
    "                    bug_locations[i] = 1\n",
    "                    \n",
    "    return bug_locations\n",
    "\n",
    "# 2. Prepare paired data with token-level annotations\n",
    "def prepare_paired_data(buggy_tokens_list: List[List[str]], fixed_tokens_list: List[List[str]]):\n",
    "    \"\"\"\n",
    "    Prepare paired data with token-level bug annotations.\n",
    "    Returns file-level labels and token-level labels.\n",
    "    \"\"\"\n",
    "    paired_data = []\n",
    "    \n",
    "    for i, (buggy_tokens, fixed_tokens) in enumerate(zip(buggy_tokens_list, fixed_tokens_list)):\n",
    "        # Skip empty files\n",
    "        if not buggy_tokens or not fixed_tokens:\n",
    "            continue\n",
    "            \n",
    "        # Find bug locations\n",
    "        token_level_labels = identify_bug_locations(buggy_tokens, fixed_tokens)\n",
    "        \n",
    "        # Determine if file is buggy (if any token is marked as buggy)\n",
    "        file_is_buggy = 1 if sum(token_level_labels) > 0 else 0\n",
    "        \n",
    "        paired_data.append({\n",
    "            'id': i,\n",
    "            'buggy_tokens': buggy_tokens,\n",
    "            'token_level_labels': token_level_labels,\n",
    "            'file_is_buggy': file_is_buggy\n",
    "        })\n",
    "        \n",
    "    return paired_data\n",
    "\n",
    "# 3. Build vocabulary\n",
    "def build_vocabulary(token_sequences: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"Build a vocabulary from all tokens.\"\"\"\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for tokens in token_sequences:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# 4. Convert and pad sequences\n",
    "def prepare_sequences(data_items, vocab: Dict[str, int], max_len: int):\n",
    "    \"\"\"Convert token sequences and labels to padded numerical arrays.\"\"\"\n",
    "    token_data = []\n",
    "    file_labels = []\n",
    "    token_labels = []\n",
    "    \n",
    "    for item in data_items:\n",
    "        tokens = item['buggy_tokens']\n",
    "        token_level_label = item['token_level_labels']\n",
    "        \n",
    "        # Process tokens (truncate or pad)\n",
    "        if len(tokens) > max_len:\n",
    "            token_indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens[:max_len]]\n",
    "            item_token_labels = token_level_label[:max_len]\n",
    "        else:\n",
    "            token_indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "            item_token_labels = token_level_label.copy()\n",
    "            # Pad tokens and labels\n",
    "            padding_length = max_len - len(tokens)\n",
    "            token_indices += [vocab[\"<PAD>\"]] * padding_length\n",
    "            item_token_labels += [0] * padding_length  # Pad labels with 0 (non-buggy)\n",
    "            \n",
    "        token_data.append(token_indices)\n",
    "        file_labels.append(item['file_is_buggy'])\n",
    "        token_labels.append(item_token_labels)\n",
    "    \n",
    "    return np.array(token_data), np.array(file_labels), np.array(token_labels)\n",
    "\n",
    "# 5. Custom Dataset for Bug Localization\n",
    "class BugLocalizationDataset(Dataset):\n",
    "    def __init__(self, token_features, file_labels, token_labels):\n",
    "        self.token_features = torch.tensor(token_features, dtype=torch.long)\n",
    "        self.file_labels = torch.tensor(file_labels, dtype=torch.float)\n",
    "        self.token_labels = torch.tensor(token_labels, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.token_features[idx], self.file_labels[idx], self.token_labels[idx]\n",
    "\n",
    "# 6. Bug Localization Model (with both file-level and token-level predictions)\n",
    "class BugLocalizationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BugLocalizationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Token-level classification\n",
    "        self.token_classifier = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # File-level classification\n",
    "        self.file_classifier = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Token-level predictions\n",
    "        token_logits = self.token_classifier(lstm_out)  # [batch_size, seq_len, 1]\n",
    "        token_probs = self.sigmoid(token_logits.squeeze(-1))  # [batch_size, seq_len]\n",
    "        \n",
    "        # File-level prediction (using the final hidden state)\n",
    "        hidden = hidden.transpose(0, 1).contiguous().view(x.size(0), -1)  # [batch_size, hidden_dim]\n",
    "        file_logit = self.file_classifier(hidden)  # [batch_size, 1]\n",
    "        file_prob = self.sigmoid(file_logit).squeeze(-1)  # [batch_size]\n",
    "        \n",
    "        return file_prob, token_probs\n",
    "\n",
    "# 7. Split data and train model\n",
    "def split_and_train_bug_localization(buggy_tokens_list, fixed_tokens_list, max_len=500, test_size=0.2):\n",
    "    \"\"\"Split data and train a bug localization model with both file and token-level labels.\"\"\"\n",
    "    # Prepare paired data\n",
    "    paired_data = prepare_paired_data(buggy_tokens_list, fixed_tokens_list)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    train_data, test_data = train_test_split(\n",
    "        paired_data, test_size=test_size, random_state=42,\n",
    "        # Stratify based on file_is_buggy to maintain class balance\n",
    "        stratify=[item['file_is_buggy'] for item in paired_data]\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(train_data)} files\")\n",
    "    print(f\"Testing set: {len(test_data)} files\")\n",
    "    \n",
    "    # Extract token sequences for vocabulary building\n",
    "    train_token_sequences = [item['buggy_tokens'] for item in train_data]\n",
    "    \n",
    "    # Build vocabulary from training data only\n",
    "    vocab = build_vocabulary(train_token_sequences)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    # Prepare sequences for training and testing\n",
    "    train_features, train_file_labels, train_token_labels = prepare_sequences(train_data, vocab, max_len)\n",
    "    test_features, test_file_labels, test_token_labels = prepare_sequences(test_data, vocab, max_len)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BugLocalizationDataset(train_features, train_file_labels, train_token_labels)\n",
    "    test_dataset = BugLocalizationDataset(test_features, test_file_labels, test_token_labels)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Initialize model\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 64\n",
    "    hidden_dim = 128\n",
    "    \n",
    "    model = BugLocalizationModel(vocab_size, embedding_dim, hidden_dim)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    file_criterion = nn.BCELoss()\n",
    "    token_criterion = nn.BCELoss(reduction='none')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for tokens, file_labels, token_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            file_preds, token_preds = model(tokens)\n",
    "            \n",
    "            # Calculate file-level loss\n",
    "            file_loss = file_criterion(file_preds, file_labels)\n",
    "            \n",
    "            # Calculate token-level loss with masking for padding\n",
    "            mask = (tokens != 0).float()\n",
    "            token_loss = token_criterion(token_preds, token_labels)\n",
    "            masked_token_loss = (token_loss * mask).sum() / mask.sum().clamp(min=1e-5)\n",
    "            \n",
    "            # Combined loss (with weighting)\n",
    "            combined_loss = file_loss + masked_token_loss\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += combined_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            file_correct = 0\n",
    "            file_total = 0\n",
    "            token_metrics = {\n",
    "                'true_pos': 0, 'false_pos': 0, 'true_neg': 0, 'false_neg': 0\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for tokens, file_labels, token_labels in test_loader:\n",
    "                    file_preds, token_preds = model(tokens)\n",
    "                    \n",
    "                    # File-level metrics\n",
    "                    file_pred_labels = (file_preds > 0.5).float()\n",
    "                    file_correct += (file_pred_labels == file_labels).sum().item()\n",
    "                    file_total += len(file_labels)\n",
    "                    \n",
    "                    # Token-level metrics with mask for padding\n",
    "                    mask = (tokens != 0).float()\n",
    "                    token_pred_labels = (token_preds > 0.5).float()\n",
    "                    \n",
    "                    # Calculate token metrics\n",
    "                    token_metrics['true_pos'] += ((token_pred_labels == 1) & (token_labels == 1) & (mask == 1)).sum().item()\n",
    "                    token_metrics['false_pos'] += ((token_pred_labels == 1) & (token_labels == 0) & (mask == 1)).sum().item()\n",
    "                    token_metrics['true_neg'] += ((token_pred_labels == 0) & (token_labels == 0) & (mask == 1)).sum().item()\n",
    "                    token_metrics['false_neg'] += ((token_pred_labels == 0) & (token_labels == 1) & (mask == 1)).sum().item()\n",
    "            \n",
    "            # Calculate file-level accuracy\n",
    "            file_accuracy = file_correct / file_total\n",
    "            \n",
    "            # Calculate token-level metrics\n",
    "            token_precision = token_metrics['true_pos'] / (token_metrics['true_pos'] + token_metrics['false_pos']) if (token_metrics['true_pos'] + token_metrics['false_pos']) > 0 else 0\n",
    "            token_recall = token_metrics['true_pos'] / (token_metrics['true_pos'] + token_metrics['false_neg']) if (token_metrics['true_pos'] + token_metrics['false_neg']) > 0 else 0\n",
    "            token_f1 = 2 * (token_precision * token_recall) / (token_precision + token_recall) if (token_precision + token_recall) > 0 else 0\n",
    "            \n",
    "            print(f\"File-level Accuracy: {file_accuracy:.4f}\")\n",
    "            print(f\"Token-level Precision: {token_precision:.4f}, Recall: {token_recall:.4f}, F1: {token_f1:.4f}\")\n",
    "    \n",
    "    return model, vocab, test_data\n",
    "\n",
    "# 8. Function to predict bugs in a new file\n",
    "def predict_bugs_in_file(model, vocab, file_tokens, max_len=500, threshold=0.5):\n",
    "    \"\"\"Predict if a file has bugs and identify their locations.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and pad\n",
    "    if len(file_tokens) > max_len:\n",
    "        indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in file_tokens[:max_len]]\n",
    "        effective_len = max_len\n",
    "    else:\n",
    "        indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in file_tokens]\n",
    "        effective_len = len(indices)\n",
    "        indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        file_prob, token_probs = model(tensor)\n",
    "        file_is_buggy = file_prob.item() > threshold\n",
    "        \n",
    "    # Get token-level predictions for original (non-padded) tokens\n",
    "    token_predictions = []\n",
    "    for i, (token, prob) in enumerate(zip(file_tokens[:effective_len], token_probs[0][:effective_len])):\n",
    "        if prob.item() > threshold:\n",
    "            token_predictions.append((i, token, prob.item()))\n",
    "    \n",
    "    return {\n",
    "        'file_is_buggy': file_is_buggy,\n",
    "        'file_bug_probability': file_prob.item(),\n",
    "        'bug_locations': token_predictions\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "#model, vocab, test_data = split_and_train_bug_localization(buggy_tokens, fixed_tokens)\n",
    "# \n",
    "# # Save the model\n",
    "\"\"\"\"\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab\n",
    "}, 'bug_localization_model.pt')\n",
    "\"\"\"\n",
    "\"\"\"\"\n",
    "buggy_tokens = tokenize_defects4j_project(\"/tmp/Lang_1_b/\") # Bug: uses - instead of +\n",
    "fixed_tokens = [\"def\", \"add\", \"(\", \"a\", \",\", \"b\", \")\", \":\", \"return\", \"a\", \"+\", \"b\"]\n",
    "\n",
    "# Call the function to identify and print bugs\n",
    "bug_labels = identify_bug_locations(buggy_tokens, fixed_tokens)\n",
    "print(bug_labels)\n",
    "# # Example: Test on the first test item\n",
    "if test_data:\n",
    "    for i in range(0,48):\n",
    "        test_file = test_data[i]\n",
    "        result = predict_bugs_in_file(model, vocab, test_file['buggy_tokens'])\n",
    "    #     \n",
    "        print(f\"File is buggy: {result['file_is_buggy']} (Probability: {result['file_bug_probability']:.4f})\")\n",
    "        print(f\"Found {len(result['bug_locations'])} potential bug locations\")\n",
    "    #     \n",
    "        if result['bug_locations']:\n",
    "            print(\"\\nPotential bugs:\")\n",
    "            for idx, token, prob in result['bug_locations']:\n",
    "                print(f\"Token {idx}: '{token}' (probability: {prob:.4f})\")\n",
    "\"\"\"\n",
    "#bug = tokenize_java_file(\"/tmp/Math_35_b/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\")\n",
    "#fix = tokenize_java_file(\"/tmp/Math_35_f/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\")\n",
    "#list = identify_bug_locations(bug,fix)\n",
    "#print(list)\n",
    "#if 1 in list:\n",
    "    #print(\"bug\")\n",
    "buggy_path = \"/tmp/Math_35_b/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\"\n",
    "fixed_path = \"/tmp/Math_35_f/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\"\n",
    "\n",
    "# Read files\n",
    "with open(buggy_path, 'r') as f1, open(fixed_path, 'r') as f2:\n",
    "    buggy_lines = f1.readlines()\n",
    "    fixed_lines = f2.readlines()\n",
    "\n",
    "# Compute differences\n",
    "diff = difflib.unified_diff(buggy_lines, fixed_lines, lineterm='')\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\".join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buggy version has 1196 Java files.\n",
      "Fixed version has 1196 Java files.\n",
      "Comparing 1196 files found in both versions.\n",
      "Potential bugs found in src/main/java/org/apache/commons/math3/distribution/MultivariateNormalDistribution.java at token indices: [639, 640, 642, 643]\n"
     ]
    }
   ],
   "source": [
    "#example for comparing individual directories\n",
    "import os\n",
    "import difflib\n",
    "import javalang\n",
    "from typing import List\n",
    "\n",
    "def get_java_files(directory):\n",
    "    \"\"\"Recursively get all Java file paths in a directory.\"\"\"\n",
    "    java_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".java\"):\n",
    "                java_files.append(os.path.join(root, file))\n",
    "    return java_files\n",
    "\n",
    "def get_relative_paths(files, base_dir):\n",
    "    \"\"\"Convert absolute file paths to relative paths for comparison.\"\"\"\n",
    "    return {os.path.relpath(f, base_dir): f for f in files}\n",
    "\n",
    "def tokenize_java_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Tokenize a single Java file using javalang.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        java_code = f.read()\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(java_code))\n",
    "        return [token.value for token in tokens]\n",
    "    except javalang.tokenizer.LexerError:\n",
    "        print(f\"Lexer error in file: {file_path} (possibly invalid Java syntax)\")\n",
    "        return []\n",
    "\n",
    "def find_bug_locations(buggy_tokens, fixed_tokens):\n",
    "    \"\"\"Find locations where tokens differ.\"\"\"\n",
    "    diff = list(difflib.ndiff(buggy_tokens, fixed_tokens))\n",
    "    changes = [i for i, line in enumerate(diff) if line.startswith('- ') or line.startswith('+ ')]\n",
    "    return changes  # Returns indices of differences\n",
    "\n",
    "# Debugging: Check dictionary structure\n",
    "buggy_dir = checkout_version(\"Math\",\"11\",\"b\")  # Make sure this function is defined correctly\n",
    "fixed_dir = checkout_version(\"Math\", \"11\",\"f\")  # Same for this one\n",
    "\n",
    "buggy_files = get_java_files(buggy_dir)\n",
    "fixed_files = get_java_files(fixed_dir)\n",
    "\n",
    "print(f\"Buggy version has {len(buggy_files)} Java files.\")\n",
    "print(f\"Fixed version has {len(fixed_files)} Java files.\")\n",
    "\n",
    "buggy_rel = get_relative_paths(buggy_files, buggy_dir)\n",
    "fixed_rel = get_relative_paths(fixed_files, fixed_dir)\n",
    "\n",
    "common_files = set(buggy_rel.keys()) & set(fixed_rel.keys())\n",
    "\n",
    "print(f\"Comparing {len(common_files)} files found in both versions.\")\n",
    "\n",
    "# Tokenize the common files\n",
    "buggy_tokens = {file: tokenize_java_file(buggy_rel[file]) for file in common_files}\n",
    "fixed_tokens = {file: tokenize_java_file(fixed_rel[file]) for file in common_files}\n",
    "\n",
    "# Compare the tokens and find bug locations\n",
    "bug_locations = {file: find_bug_locations(buggy_tokens[file], fixed_tokens[file]) for file in common_files}\n",
    "\n",
    "# Print bug locations for each file\n",
    "for file, changes in bug_locations.items():\n",
    "    if changes:\n",
    "        print(f\"Potential bugs found in {file} at token indices: {changes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Math-13...\n",
      "Checking 1195 files found in both versions...\n",
      "Found 1 files with different hashes.\n",
      "Found 1 files with changes, totaling 2 changes.\n",
      "\n",
      "File: src/main/java/org/apache/commons/math3/optimization/general/AbstractLeastSquaresOptimizer.java\n",
      "  Change 1: added at line 562\n",
      "    In class: AbstractLeastSquaresOptimizer\n",
      "    In method: squareRoot()\n",
      "    Buggy: \n",
      "    Fixed: if ( m instanceof DiagonalMatrix ) { final int dim...\n",
      "  Change 2: added at line None\n",
      "    Buggy: \n",
      "    Fixed: }\n",
      "\n",
      "HTML report generated: Math-13_bug_report.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import difflib\n",
    "import javalang\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class CodeChange:\n",
    "    \"\"\"Represents a code change between buggy and fixed versions.\"\"\"\n",
    "    file_path: str\n",
    "    line_number: Optional[int]  # May be None for structural changes\n",
    "    change_type: str  # 'added', 'deleted', 'modified'\n",
    "    buggy_code: str\n",
    "    fixed_code: str\n",
    "    context_before: str\n",
    "    context_after: str\n",
    "    method_name: Optional[str]  # Method containing the change\n",
    "    class_name: Optional[str]   # Class containing the change\n",
    "\n",
    "def get_java_files(directory: str) -> List[str]:\n",
    "    \"\"\"Recursively get all Java file paths in a directory.\"\"\"\n",
    "    return [str(p) for p in Path(directory).glob('**/*.java')]\n",
    "\n",
    "def get_relative_paths(files: List[str], base_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"Convert absolute file paths to relative paths for comparison.\"\"\"\n",
    "    return {os.path.relpath(f, base_dir): f for f in files}\n",
    "\n",
    "def compute_file_hash(file_path: str) -> str:\n",
    "    \"\"\"Compute a hash of the file contents.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def find_changed_files(buggy_dir: str, fixed_dir: str) -> Dict[str, Tuple[str, str]]:\n",
    "    \"\"\"Find files that differ between buggy and fixed versions based on hash.\"\"\"\n",
    "    buggy_files = get_java_files(buggy_dir)\n",
    "    fixed_files = get_java_files(fixed_dir)\n",
    "    \n",
    "    buggy_rel = get_relative_paths(buggy_files, buggy_dir)\n",
    "    fixed_rel = get_relative_paths(fixed_files, fixed_dir)\n",
    "    \n",
    "    common_files = set(buggy_rel.keys()) & set(fixed_rel.keys())\n",
    "    changed_files = {}\n",
    "    \n",
    "    print(f\"Checking {len(common_files)} files found in both versions...\")\n",
    "    \n",
    "    # First filter: use file hash comparison to identify changed files\n",
    "    for file in common_files:\n",
    "        buggy_hash = compute_file_hash(buggy_rel[file])\n",
    "        fixed_hash = compute_file_hash(fixed_rel[file])\n",
    "        \n",
    "        if buggy_hash != fixed_hash:\n",
    "            changed_files[file] = (buggy_rel[file], fixed_rel[file])\n",
    "    \n",
    "    print(f\"Found {len(changed_files)} files with different hashes.\")\n",
    "    return changed_files\n",
    "\n",
    "def get_file_content(file_path: str) -> str:\n",
    "    \"\"\"Safely read file content handling different encodings.\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Warning: Could not decode {file_path} with any of the attempted encodings.\")\n",
    "    return \"\"\n",
    "\n",
    "def get_line_mapping(tokens: List) -> Dict[int, int]:\n",
    "    \"\"\"Create mapping from token index to line number.\"\"\"\n",
    "    return {i: token.position[0] for i, token in enumerate(tokens) if hasattr(token, 'position') and token.position}\n",
    "\n",
    "def extract_ast_info(file_path: str) -> Dict[int, Dict[str, str]]:\n",
    "    \"\"\"Extract class and method information from Java file.\"\"\"\n",
    "    content = get_file_content(file_path)\n",
    "    line_info = {}\n",
    "    \n",
    "    try:\n",
    "        tree = javalang.parse.parse(content)\n",
    "        \n",
    "        # Process classes\n",
    "        for path, node in tree.filter(javalang.tree.ClassDeclaration):\n",
    "            start_line = node.position[0] if node.position else 0\n",
    "            end_line = 0  # We'll approximate the end line\n",
    "            \n",
    "            for member in node.body:\n",
    "                if hasattr(member, 'position') and member.position:\n",
    "                    if member.position[0] > end_line:\n",
    "                        end_line = member.position[0]\n",
    "            \n",
    "            # If we couldn't find an end line, use start + 100 as an approximation\n",
    "            if end_line <= start_line:\n",
    "                end_line = start_line + 100\n",
    "            \n",
    "            for line in range(start_line, end_line + 1):\n",
    "                line_info[line] = {\n",
    "                    'class_name': node.name,\n",
    "                    'method_name': None\n",
    "                }\n",
    "        \n",
    "        # Process methods\n",
    "        for path, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "            if node.position:\n",
    "                start_line = node.position[0]\n",
    "                # Approximate method end based on body\n",
    "                end_line = start_line + 30  # Reasonable default if we can't determine\n",
    "                \n",
    "                # Try to determine class name from path\n",
    "                class_name = None\n",
    "                for p in path:\n",
    "                    if isinstance(p, javalang.tree.ClassDeclaration):\n",
    "                        class_name = p.name\n",
    "                        break\n",
    "                \n",
    "                for line in range(start_line, end_line + 1):\n",
    "                    line_info[line] = {\n",
    "                        'class_name': class_name,\n",
    "                        'method_name': node.name\n",
    "                    }\n",
    "        \n",
    "        return line_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting AST info from {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def tokenize_java_file(file_path: str) -> Tuple[List[str], List]:\n",
    "    \"\"\"Tokenize a Java file and return token values and original tokens.\"\"\"\n",
    "    # Rest of the function remains the same\n",
    "    content = get_file_content(file_path)\n",
    "    \n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(content))\n",
    "        return [token.value for token in tokens], tokens\n",
    "    except javalang.tokenizer.LexerError:\n",
    "        print(f\"Lexer error in file: {file_path} (possibly invalid Java syntax)\")\n",
    "        return [], []\n",
    "\n",
    "def analyze_file_differences(file_rel_path: str, buggy_path: str, fixed_path: str) -> List[CodeChange]:\n",
    "    \"\"\"Analyze differences between buggy and fixed versions of a file.\"\"\"\n",
    "    buggy_tokens_values, buggy_tokens = tokenize_java_file(buggy_path)\n",
    "    fixed_tokens_values, fixed_tokens = tokenize_java_file(fixed_path)\n",
    "    \n",
    "    if not buggy_tokens_values or not fixed_tokens_values:\n",
    "        return []\n",
    "    \n",
    "    # Get line number mapping\n",
    "    buggy_line_map = get_line_mapping(buggy_tokens)\n",
    "    \n",
    "    # Get AST information\n",
    "    buggy_ast_info = extract_ast_info(buggy_path)\n",
    "    \n",
    "    # Use difflib to find differences\n",
    "    matcher = difflib.SequenceMatcher(None, buggy_tokens_values, fixed_tokens_values)\n",
    "    \n",
    "    changes = []\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag != 'equal':\n",
    "            # Determine line number (from buggy version)\n",
    "            line_number = buggy_line_map.get(i1) if i1 < len(buggy_line_map) else None\n",
    "            \n",
    "            # Get context (5 tokens before and after)\n",
    "            context_before_start = max(0, i1 - 5)\n",
    "            context_before = \" \".join(buggy_tokens_values[context_before_start:i1])\n",
    "            \n",
    "            context_after_end = min(len(buggy_tokens_values), i2 + 5)\n",
    "            context_after = \" \".join(buggy_tokens_values[i2:context_after_end])\n",
    "            \n",
    "            # Get class and method information\n",
    "            class_name = None\n",
    "            method_name = None\n",
    "            if line_number and line_number in buggy_ast_info:\n",
    "                info = buggy_ast_info[line_number]\n",
    "                class_name = info.get('class_name')\n",
    "                method_name = info.get('method_name')\n",
    "            \n",
    "            # Determine change type\n",
    "            if tag == 'replace':\n",
    "                change_type = 'modified'\n",
    "            elif tag == 'delete':\n",
    "                change_type = 'deleted'\n",
    "            elif tag == 'insert':\n",
    "                change_type = 'added'\n",
    "            \n",
    "            # Get the actual code snippets\n",
    "            buggy_code = \" \".join(buggy_tokens_values[i1:i2]) if i1 < i2 else \"\"\n",
    "            fixed_code = \" \".join(fixed_tokens_values[j1:j2]) if j1 < j2 else \"\"\n",
    "            \n",
    "            changes.append(CodeChange(\n",
    "                file_path=file_rel_path,\n",
    "                line_number=line_number,\n",
    "                change_type=change_type,\n",
    "                buggy_code=buggy_code,\n",
    "                fixed_code=fixed_code,\n",
    "                context_before=context_before,\n",
    "                context_after=context_after,\n",
    "                method_name=method_name,\n",
    "                class_name=class_name\n",
    "            ))\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def analyze_project_differences(buggy_dir: str, fixed_dir: str) -> Dict[str, List[CodeChange]]:\n",
    "    \"\"\"Analyze differences between buggy and fixed versions of a project.\"\"\"\n",
    "    changed_files = find_changed_files(buggy_dir, fixed_dir)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Sequential processing instead of parallel\n",
    "    for file_rel, (buggy_path, fixed_path) in changed_files.items():\n",
    "        try:\n",
    "            changes = analyze_file_differences(file_rel, buggy_path, fixed_path)\n",
    "            if changes:\n",
    "                results[file_rel] = changes\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {file_rel}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_html_report(project_name: str, bug_id: str, changes: Dict[str, List[CodeChange]]) -> str:\n",
    "    \"\"\"Generate an HTML report of the changes.\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Bug Analysis: {project_name}-{bug_id}</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .file {{ margin-bottom: 30px; border: 1px solid #ddd; border-radius: 5px; overflow: hidden; }}\n",
    "            .file-header {{ background-color: #f5f5f5; padding: 10px; border-bottom: 1px solid #ddd; }}\n",
    "            .change {{ margin: 10px; padding: 10px; border: 1px solid #eee; border-radius: 5px; }}\n",
    "            .change-header {{ font-weight: bold; margin-bottom: 10px; }}\n",
    "            .deleted {{ background-color: #ffecec; }}\n",
    "            .added {{ background-color: #eaffea; }}\n",
    "            .modified {{ background-color: #ececff; }}\n",
    "            .code {{ font-family: monospace; white-space: pre-wrap; padding: 10px; background-color: #f9f9f9; }}\n",
    "            .context {{ color: #888; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Bug Analysis: {project_name}-{bug_id}</h1>\n",
    "        <p>Total files changed: {len(changes)}</p>\n",
    "    \"\"\"\n",
    "    \n",
    "    for file_path, file_changes in changes.items():\n",
    "        html += f\"\"\"\n",
    "        <div class=\"file\">\n",
    "            <div class=\"file-header\">\n",
    "                <h2>{file_path}</h2>\n",
    "                <p>Total changes: {len(file_changes)}</p>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        for change in file_changes:\n",
    "            html += f\"\"\"\n",
    "            <div class=\"change {change.change_type}\">\n",
    "                <div class=\"change-header\">\n",
    "                    {change.change_type.capitalize()} at line {change.line_number or 'unknown'}\n",
    "                    {f' in {change.class_name}' if change.class_name else ''}\n",
    "                    {f'.{change.method_name}()' if change.method_name else ''}\n",
    "                </div>\n",
    "                <div class=\"context\">Context before: {change.context_before}</div>\n",
    "                <div class=\"code buggy\">Buggy code: {change.buggy_code}</div>\n",
    "                <div class=\"code fixed\">Fixed code: {change.fixed_code}</div>\n",
    "                <div class=\"context\">Context after: {change.context_after}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html += \"</div>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "def main(project_name: str, bug_id: str, buggy_dir: str, fixed_dir: str):\n",
    "    \"\"\"Main function to analyze differences between buggy and fixed versions.\"\"\"\n",
    "    print(f\"Analyzing {project_name}-{bug_id}...\")\n",
    "    \n",
    "    # Analyze differences\n",
    "    changes = analyze_project_differences(buggy_dir, fixed_dir)\n",
    "    \n",
    "    # Print summary\n",
    "    total_changes = sum(len(file_changes) for file_changes in changes.values())\n",
    "    print(f\"Found {len(changes)} files with changes, totaling {total_changes} changes.\")\n",
    "    \n",
    "    for file_path, file_changes in changes.items():\n",
    "        print(f\"\\nFile: {file_path}\")\n",
    "        for i, change in enumerate(file_changes):\n",
    "            print(f\"  Change {i+1}: {change.change_type} at line {change.line_number}\")\n",
    "            if change.class_name:\n",
    "                print(f\"    In class: {change.class_name}\")\n",
    "            if change.method_name:\n",
    "                print(f\"    In method: {change.method_name}()\")\n",
    "            print(f\"    Buggy: {change.buggy_code[:50]}{'...' if len(change.buggy_code) > 50 else ''}\")\n",
    "            print(f\"    Fixed: {change.fixed_code[:50]}{'...' if len(change.fixed_code) > 50 else ''}\")\n",
    "    \n",
    "    # Generate HTML report\n",
    "    html_report = generate_html_report(project_name, bug_id, changes)\n",
    "    report_file = f\"{project_name}-{bug_id}_bug_report.html\"\n",
    "    \n",
    "    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_report)\n",
    "    \n",
    "    print(f\"\\nHTML report generated: {report_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_name = \"Math\"\n",
    "    bug_id = \"13\"\n",
    "    buggy_dir = checkout_version(project_name, bug_id, \"b\")  \n",
    "    fixed_dir = checkout_version(project_name, bug_id, \"f\") \n",
    "    \n",
    "    main(project_name, bug_id, buggy_dir, fixed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BugDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, max_length=512):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Get code snippets and context\n",
    "        code = row['code_snippet']\n",
    "        context_before = row['context_before'] if not pd.isna(row['context_before']) else \"\"\n",
    "        context_after = row['context_after'] if not pd.isna(row['context_after']) else \"\"\n",
    "        \n",
    "        # Combine for full context\n",
    "        full_snippet = f\"{context_before} {code} {context_after}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            full_snippet,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Get label (1 for buggy, 0 for fixed)\n",
    "        label = torch.tensor(1 if row['is_buggy'] else 0, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "class BugLocalizationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='microsoft/codebert-base'):\n",
    "        super(BugLocalizationModel, self).__init__()\n",
    "        self.codebert = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.codebert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.codebert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use the [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return self.sigmoid(logits)\n",
    "\n",
    "def prepare_data_from_changes(change_dict):\n",
    "    \"\"\"Convert code changes to training data format\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for file_path, changes in change_dict.items():\n",
    "        for change in changes:\n",
    "            # Add buggy version as positive example\n",
    "            if change.buggy_code.strip():\n",
    "                data.append({\n",
    "                    'file_path': file_path,\n",
    "                    'line_number': change.line_number,\n",
    "                    'code_snippet': change.buggy_code,\n",
    "                    'context_before': change.context_before,\n",
    "                    'context_after': change.context_after,\n",
    "                    'class_name': change.class_name,\n",
    "                    'method_name': change.method_name,\n",
    "                    'is_buggy': True\n",
    "                })\n",
    "            \n",
    "            # Add fixed version as negative example\n",
    "            if change.fixed_code.strip():\n",
    "                data.append({\n",
    "                    'file_path': file_path,\n",
    "                    'line_number': change.line_number,\n",
    "                    'code_snippet': change.fixed_code,\n",
    "                    'context_before': change.context_before,\n",
    "                    'context_after': change.context_after,\n",
    "                    'class_name': change.class_name,\n",
    "                    'method_name': change.method_name,\n",
    "                    'is_buggy': False\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def collect_training_data(projects_dir, output_file='bug_dataset.csv'):\n",
    "    \"\"\"Collect training data from multiple projects and bugs\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for project_dir in os.listdir(projects_dir):\n",
    "        project_path = os.path.join(projects_dir, project_dir)\n",
    "        if not os.path.isdir(project_path):\n",
    "            continue\n",
    "        \n",
    "        project_name = project_dir.split('-')[0]\n",
    "        bug_id = project_dir.split('-')[1]\n",
    "        \n",
    "        buggy_dir = os.path.join(project_path, 'buggy')\n",
    "        fixed_dir = os.path.join(project_path, 'fixed')\n",
    "        \n",
    "        if os.path.exists(buggy_dir) and os.path.exists(fixed_dir):\n",
    "            try:\n",
    "                changes = analyze_project_differences(buggy_dir, fixed_dir)\n",
    "                df = prepare_data_from_changes(changes)\n",
    "                df['project'] = project_name\n",
    "                df['bug_id'] = bug_id\n",
    "                all_data.append(df)\n",
    "                print(f\"Processed {project_name}-{bug_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {project_name}-{bug_id}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved dataset with {len(final_df)} records to {output_file}\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data collected.\")\n",
    "        return None\n",
    "\n",
    "def train_model(train_loader, val_loader, model, device, num_epochs=3, learning_rate=2e-5):\n",
    "    \"\"\"Train the bug localization model\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_preds.extend(outputs.view(-1).cpu().numpy())\n",
    "                val_true.extend(labels.view(-1).cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_preds_binary = [1 if p >= 0.5 else 0 for p in val_preds]\n",
    "        accuracy = sum(p == t for p, t in zip(val_preds_binary, val_true)) / len(val_true)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_bug_localization_model.pt\")\n",
    "            print(\"Saved best model.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_bugs(model, tokenizer, code_files, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict potential bugs in code files\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        code_files: Dictionary of file_path -> code_content\n",
    "        device: Device to run inference on\n",
    "        threshold: Probability threshold for bug prediction\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of file_path -> list of potential bug lines\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    for file_path, content in code_files.items():\n",
    "        # Split code into chunks (e.g., methods or smaller segments)\n",
    "        lines = content.split('\\n')\n",
    "        chunks = []\n",
    "        \n",
    "        # A simple chunking strategy - this could be improved to be more method-aware\n",
    "        current_chunk = []\n",
    "        for i, line in enumerate(lines):\n",
    "            current_chunk.append(line)\n",
    "            if len(current_chunk) >= 15 or i == len(lines) - 1:  # Chunk size of ~15 lines\n",
    "                chunks.append(('\\n'.join(current_chunk), i - len(current_chunk) + 1))\n",
    "                current_chunk = []\n",
    "        \n",
    "        file_bugs = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk_text, start_line in chunks:\n",
    "            encoding = tokenizer(\n",
    "                chunk_text,\n",
    "                return_tensors='pt',\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids = encoding['input_ids'].to(device)\n",
    "                attention_mask = encoding['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                prob = outputs.squeeze().cpu().item()\n",
    "                \n",
    "                if prob >= threshold:\n",
    "                    # If chunk is predicted as buggy, add it to results\n",
    "                    file_bugs.append({\n",
    "                        'start_line': start_line,\n",
    "                        'end_line': start_line + len(chunk_text.split('\\n')) - 1,\n",
    "                        'bug_probability': prob,\n",
    "                        'chunk': chunk_text\n",
    "                    })\n",
    "        \n",
    "        if file_bugs:\n",
    "            results[file_path] = file_bugs\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def checkout_version(project_name, bug_id, version_type):\n",
    "    \"\"\"\n",
    "    Check out a specific version of a project using Defects4J or similar.\n",
    "    \n",
    "    Args:\n",
    "        project_name: Name of the project (e.g., 'Math', 'Lang')\n",
    "        bug_id: Bug identifier (e.g., '1', '10')\n",
    "        version_type: 'b' for buggy version, 'f' for fixed version\n",
    "    \n",
    "    Returns:\n",
    "        Path to the checked out code\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp(prefix=f\"{project_name}-{bug_id}-{version_type}\")\n",
    "    \n",
    "    # Map version type to Defects4J version flag\n",
    "    version_flag = \"b\" if version_type == \"b\" else \"f\"\n",
    "    \n",
    "    try:\n",
    "        # Run Defects4J checkout command\n",
    "        cmd = [\n",
    "            \"defects4j\", \"checkout\",\n",
    "            \"-p\", project_name,\n",
    "            \"-v\", f\"{bug_id}{version_flag}\",\n",
    "            \"-w\", temp_dir\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        print(f\"Checked out {project_name}-{bug_id} {version_type} version to {temp_dir}\")\n",
    "        \n",
    "        return temp_dir\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error checking out {project_name}-{bug_id} {version_type} version:\")\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "        print(f\"Error: {e.stderr}\")\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        raise\n",
    "\n",
    "def collect_training_data_from_defects4j(projects, bug_ids, output_file='bug_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Collect training data from multiple Defects4J projects and bugs\n",
    "    \n",
    "    Args:\n",
    "        projects: List of project names (e.g., ['Math', 'Lang'])\n",
    "        bug_ids: Dictionary mapping project names to lists of bug IDs\n",
    "        output_file: Path to save the collected data\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with collected data\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for project_name in projects:\n",
    "        for bug_id in bug_ids.get(project_name, []):\n",
    "            try:\n",
    "                print(f\"Processing {project_name}-{bug_id}...\")\n",
    "                \n",
    "                # Check out buggy and fixed versions\n",
    "                buggy_dir = checkout_version(project_name, bug_id, \"b\")\n",
    "                fixed_dir = checkout_version(project_name, bug_id, \"f\")\n",
    "                \n",
    "                # Analyze differences\n",
    "                changes = analyze_project_differences(buggy_dir, fixed_dir)\n",
    "                \n",
    "                # Prepare data\n",
    "                df = prepare_data_from_changes(changes)\n",
    "                df['project'] = project_name\n",
    "                df['bug_id'] = bug_id\n",
    "                all_data.append(df)\n",
    "                \n",
    "                print(f\"Successfully processed {project_name}-{bug_id}\")\n",
    "                \n",
    "                # Clean up checkout directories\n",
    "                shutil.rmtree(buggy_dir, ignore_errors=True)\n",
    "                shutil.rmtree(fixed_dir, ignore_errors=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {project_name}-{bug_id}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved dataset with {len(final_df)} records to {output_file}\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data collected.\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define projects and bug IDs to use\n",
    "    projects = ['Math', 'Lang', 'Time', 'Chart']\n",
    "    bug_ids = {\n",
    "        'Math': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n",
    "        'Lang': ['1', '2', '3', '4', '5'],\n",
    "        'Time': ['1', '2', '3', '4', '5'],\n",
    "        'Chart': ['1', '2', '3', '4', '5']\n",
    "    }\n",
    "    \n",
    "    # Parameters\n",
    "    batch_size = 8\n",
    "    max_length = 512\n",
    "    num_epochs = 3\n",
    "    learning_rate = 2e-5\n",
    "    \n",
    "    # 1. Collect and prepare data\n",
    "    print(\"Collecting training data...\")\n",
    "    if os.path.exists(\"bug_dataset.csv\"):\n",
    "        print(\"Loading existing dataset...\")\n",
    "        data_df = pd.read_csv(\"bug_dataset.csv\")\n",
    "    else:\n",
    "        data_df = collect_training_data_from_defects4j(projects, bug_ids)\n",
    "        if data_df is None:\n",
    "            print(\"Failed to collect data. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    # Rest of the training code remains the same\n",
    "    # ...\n",
    "def generate_prediction_report(project_name, bug_id, predictions, actual_changes, output_path):\n",
    "    \"\"\"Generate HTML report comparing predictions to actual bugs\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Bug Prediction Report: {project_name}-{bug_id}</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .file {{ margin-bottom: 30px; border: 1px solid #ddd; border-radius: 5px; overflow: hidden; }}\n",
    "            .file-header {{ background-color: #f5f5f5; padding: 10px; border-bottom: 1px solid #ddd; }}\n",
    "            .prediction {{ margin: 10px; padding: 10px; border: 1px solid #eee; border-radius: 5px; background-color: #fff8e6; }}\n",
    "            .actual {{ margin: 10px; padding: 10px; border: 1px solid #eee; border-radius: 5px; background-color: #e6fff0; }}\n",
    "            .match {{ background-color: #d1ffdd; border: 2px solid #28a745; }}\n",
    "            .code {{ font-family: monospace; white-space: pre-wrap; padding: 10px; background-color: #f9f9f9; }}\n",
    "            .metrics {{ margin-top: 20px; padding: 10px; background-color: #f0f0f0; border-radius: 5px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Bug Prediction Report: {project_name}-{bug_id}</h1>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect all files\n",
    "    all_files = set(predictions.keys())\n",
    "    for file_path in actual_changes.keys():\n",
    "        all_files.add(file_path)\n",
    "    \n",
    "    # Track metrics\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in sorted(all_files):\n",
    "        html += f\"\"\"\n",
    "        <div class=\"file\">\n",
    "            <div class=\"file-header\">\n",
    "                <h2>{file_path}</h2>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get predicted bugs for this file\n",
    "        file_predictions = predictions.get(file_path, [])\n",
    "        predicted_lines = set()\n",
    "        for pred in file_predictions:\n",
    "            for line in range(pred['start_line'], pred['end_line'] + 1):\n",
    "                predicted_lines.add(line)\n",
    "        \n",
    "        # Get actual bugs for this file\n",
    "        file_changes = actual_changes.get(file_path, [])\n",
    "        actual_lines = set()\n",
    "        for change in file_changes:\n",
    "            if change.line_number:\n",
    "                actual_lines.add(change.line_number)\n",
    "        \n",
    "        # Show predictions\n",
    "        if file_predictions:\n",
    "            html += \"<h3>Predicted Bugs:</h3>\"\n",
    "            for pred in file_predictions:\n",
    "                # Check if prediction overlaps with actual bug\n",
    "                matches_actual = any(line in actual_lines for line in range(pred['start_line'], pred['end_line'] + 1))\n",
    "                match_class = \" match\" if matches_actual else \"\"\n",
    "                \n",
    "                if matches_actual:\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    false_positives += 1\n",
    "                \n",
    "                html += f\"\"\"\n",
    "                <div class=\"prediction{match_class}\">\n",
    "                    <p>Lines {pred['start_line']}-{pred['end_line']} (Confidence: {pred['bug_probability']:.2f})</p>\n",
    "                    <div class=\"code\">{pred['chunk']}</div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        # Show actual bugs\n",
    "        if file_changes:\n",
    "            html += \"<h3>Actual Bugs:</h3>\"\n",
    "            for change in file_changes:\n",
    "                if not change.line_number:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if actual bug was predicted\n",
    "                was_predicted = change.line_number in predicted_lines\n",
    "                match_class = \" match\" if was_predicted else \"\"\n",
    "                \n",
    "                if not was_predicted:\n",
    "                    false_negatives += 1\n",
    "                \n",
    "                html += f\"\"\"\n",
    "                <div class=\"actual{match_class}\">\n",
    "                    <p>Line {change.line_number} ({change.change_type})</p>\n",
    "                    <p>In {change.class_name}.{change.method_name if change.method_name else \"\"}</p>\n",
    "                    <div class=\"code\">Buggy: {change.buggy_code}</div>\n",
    "                    <div class=\"code\">Fixed: {change.fixed_code}</div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"</div>\"  # Close file div\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    html += f\"\"\"\n",
    "    <div class=\"metrics\">\n",
    "        <h2>Performance Metrics</h2>\n",
    "        <p>True Positives: {true_positives}</p>\n",
    "        <p>False Positives: {false_positives}</p>\n",
    "        <p>False Negatives: {false_negatives}</p>\n",
    "        <p>Precision: {precision:.2f}</p>\n",
    "        <p>Recall: {recall:.2f}</p>\n",
    "        <p>F1 Score: {f1:.2f}</p>\n",
    "    </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "# Then to use the model on a new project:\n",
    "def analyze_new_project(project_name, bug_id, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Analyze a specific project bug to predict bug locations\n",
    "    \"\"\"\n",
    "    # Check out the buggy version\n",
    "    buggy_dir = checkout_version(project_name, bug_id, \"b\")\n",
    "    \n",
    "    # Get all Java files\n",
    "    java_files = get_java_files(buggy_dir)\n",
    "    \n",
    "    # Read file contents\n",
    "    code_files = {}\n",
    "    for file_path in java_files:\n",
    "        rel_path = os.path.relpath(file_path, buggy_dir)\n",
    "        code_files[rel_path] = get_file_content(file_path)\n",
    "    \n",
    "    # Predict bugs\n",
    "    predictions = predict_bugs(model, tokenizer, code_files, device)\n",
    "    \n",
    "    # Generate report\n",
    "    report_path = f\"{project_name}-{bug_id}_bug_predictions.html\"\n",
    "    generate_prediction_report(project_name, bug_id, predictions, report_path)\n",
    "    \n",
    "    # Clean up\n",
    "    shutil.rmtree(buggy_dir, ignore_errors=True)\n",
    "    \n",
    "    return predictions, report_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bug localization example...\n",
      "Using device: cpu\n",
      "Loading existing dataset from bug_model_output/bug_dataset.csv\n",
      "Dataset contains 2 samples\n",
      "Sample distribution: Buggy=0, Non-buggy=2\n",
      "Initializing tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1 samples\n",
      "Validation set: 1 samples\n",
      "Training new model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]: 100%|| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Epoch 1/3 [Val]: 100%|| 1/1 [00:00<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.5241\n",
      "Val Loss: 0.4527, Accuracy: 1.0000\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]: 100%|| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Epoch 2/3 [Val]: 100%|| 1/1 [00:00<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Train Loss: 0.4499\n",
      "Val Loss: 0.4093, Accuracy: 1.0000\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]: 100%|| 1/1 [00:00<00:00,  1.91it/s]\n",
      "Epoch 3/3 [Val]: 100%|| 1/1 [00:00<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Train Loss: 0.4080\n",
      "Val Loss: 0.3678, Accuracy: 1.0000\n",
      "Saved best model.\n",
      "\n",
      "Testing model on Lang-33...\n",
      "Checked out Lang-33 b version to /var/folders/f2/0s8yk3ss055_ygbx2_lw3m9r0000gn/T/Lang-33-bc_qz_7l9\n",
      "Found 182 Java files\n",
      "Running predictions...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 137\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError analyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_project\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_bug_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mrun_bug_localization_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 92\u001b[0m, in \u001b[0;36mrun_bug_localization_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Predict bugs\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_bugs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 252\u001b[0m, in \u001b[0;36mpredict_bugs\u001b[0;34m(model, tokenizer, code_files, device, threshold)\u001b[0m\n\u001b[1;32m    249\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    250\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 252\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m prob \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# If chunk is predicted as buggy, add it to results\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[31], line 59\u001b[0m, in \u001b[0;36mBugLocalizationModel.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m---> 59\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Use the [CLS] token representation\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:266\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Assuming all the functions from previous examples are defined\n",
    "\n",
    "def run_bug_localization_example():\n",
    "    \"\"\"Complete example workflow for bug localization model\"\"\"\n",
    "    print(\"Starting bug localization example...\")\n",
    "    \n",
    "    # 1. Define project data\n",
    "    projects = ['Math']  # Start with just one project for the example\n",
    "    bug_ids = {'Math': ['13']}  # Using Math-13 as an example\n",
    "    \n",
    "    # 2. Set up paths and device\n",
    "    output_dir = \"bug_model_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 3. Check for existing dataset or create a new one\n",
    "    dataset_path = os.path.join(output_dir, \"bug_dataset.csv\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"Loading existing dataset from {dataset_path}\")\n",
    "        data_df = pd.read_csv(dataset_path)\n",
    "    else:\n",
    "        print(\"Creating new dataset by analyzing projects...\")\n",
    "        data_df = collect_training_data_from_defects4j(projects, bug_ids, dataset_path)\n",
    "        if data_df is None or len(data_df) == 0:\n",
    "            print(\"Failed to collect enough data. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    print(f\"Dataset contains {len(data_df)} samples\")\n",
    "    print(f\"Sample distribution: Buggy={data_df['is_buggy'].sum()}, Non-buggy={len(data_df) - data_df['is_buggy'].sum()}\")\n",
    "    \n",
    "    # 4. Prepare the tokenizer and model\n",
    "    print(\"Initializing tokenizer and model...\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    model = BugLocalizationModel().to(device)\n",
    "    \n",
    "    train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42, \n",
    "                                       stratify=data_df['is_buggy'])\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    \n",
    "    train_dataset = BugDataset(train_df, tokenizer)\n",
    "    val_dataset = BugDataset(val_df, tokenizer)\n",
    "    \n",
    "    batch_size = 8\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 6. Train model or load existing model\n",
    "    model_path = os.path.join(output_dir, \"best_bug_model.pt\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing model from {model_path}\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    else:\n",
    "        print(\"Training new model...\")\n",
    "        model = train_model(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            model,\n",
    "            device,\n",
    "            num_epochs=3,\n",
    "            learning_rate=2e-5\n",
    "        )\n",
    "    \n",
    "    # 7. Test on a new bug\n",
    "    test_project = \"Lang\"\n",
    "    test_bug_id = \"33\"\n",
    "    \n",
    "    print(f\"\\nTesting model on {test_project}-{test_bug_id}...\")\n",
    "    \n",
    "    # Check out the buggy version\n",
    "    try:\n",
    "        buggy_dir = checkout_version(test_project, test_bug_id, \"b\")\n",
    "        \n",
    "        # Get all Java files\n",
    "        java_files = get_java_files(buggy_dir)\n",
    "        print(f\"Found {len(java_files)} Java files\")\n",
    "        \n",
    "        # Read file contents\n",
    "        code_files = {}\n",
    "        for file_path in java_files:\n",
    "            rel_path = os.path.relpath(file_path, buggy_dir)\n",
    "            code_files[rel_path] = get_file_content(file_path)\n",
    "        \n",
    "        # Predict bugs\n",
    "        print(\"Running predictions...\")\n",
    "        predictions = predict_bugs(model, tokenizer, code_files, device, threshold=0.7)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        print(f\"Found potential bugs in {len(predictions)} files\")\n",
    "        \n",
    "        for file_path, bugs in predictions.items():\n",
    "            print(f\"\\nFile: {file_path}\")\n",
    "            print(f\"Number of potential bugs: {len(bugs)}\")\n",
    "            \n",
    "            # Just show first bug for brevity in example\n",
    "            if bugs:\n",
    "                bug = bugs[0]\n",
    "                print(f\"  Lines {bug['start_line']}-{bug['end_line']} (Probability: {bug['bug_probability']:.2f})\")\n",
    "                print(f\"  Code excerpt:\")\n",
    "                print(f\"  {bug['chunk'][:200]}...\")  # Show part of the code\n",
    "        \n",
    "        # Check if we found the actual bug \n",
    "        # (Requires accessing Defects4J bug information)\n",
    "        fixed_dir = checkout_version(test_project, test_bug_id, \"f\")\n",
    "        changes = analyze_project_differences(buggy_dir, fixed_dir)\n",
    "        \n",
    "        print(\"\\nActual bug locations:\")\n",
    "        for file_path, file_changes in changes.items():\n",
    "            print(f\"File: {file_path}\")\n",
    "            for change in file_changes:\n",
    "                print(f\"  Line {change.line_number}: {change.change_type}\")\n",
    "                print(f\"  Buggy: {change.buggy_code[:100]}...\")\n",
    "                print(f\"  Fixed: {change.fixed_code[:100]}...\")\n",
    "        \n",
    "        # Generate HTML report\n",
    "        report_path = os.path.join(output_dir, f\"{test_project}-{test_bug_id}_predictions.html\")\n",
    "        generate_prediction_report(test_project, test_bug_id, predictions, changes, report_path)\n",
    "        print(f\"\\nPrediction report saved to: {report_path}\")\n",
    "        \n",
    "        # Clean up\n",
    "        shutil.rmtree(buggy_dir, ignore_errors=True)\n",
    "        shutil.rmtree(fixed_dir, ignore_errors=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {test_project}-{test_bug_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_bug_localization_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing project: Lang\n",
      "  Processing bug: Lang-1\n",
      "  Processing bug: Lang-3\n",
      "  Processing bug: Lang-4\n",
      "  Processing bug: Lang-5\n",
      "  Processing bug: Lang-6\n",
      "  Processing bug: Lang-7\n",
      "  Processing bug: Lang-8\n",
      "  Processing bug: Lang-9\n",
      "  Processing bug: Lang-10\n",
      "  Processing bug: Lang-11\n",
      "  Processing bug: Lang-12\n",
      "  Processing bug: Lang-13\n",
      "  Processing bug: Lang-14\n",
      "  Processing bug: Lang-15\n",
      "  Processing bug: Lang-16\n",
      "  Processing bug: Lang-17\n",
      "  Processing bug: Lang-19\n",
      "  Processing bug: Lang-20\n",
      "  Processing bug: Lang-21\n",
      "  Processing bug: Lang-22\n",
      "  Processing bug: Lang-23\n",
      "  Processing bug: Lang-24\n",
      "  Processing bug: Lang-26\n",
      "  Processing bug: Lang-27\n",
      "  Processing bug: Lang-28\n",
      "  Processing bug: Lang-29\n",
      "  Processing bug: Lang-30\n",
      "  Processing bug: Lang-31\n",
      "  Processing bug: Lang-32\n",
      "  Processing bug: Lang-33\n",
      "  Processing bug: Lang-34\n",
      "  Processing bug: Lang-35\n",
      "  Processing bug: Lang-36\n",
      "  Processing bug: Lang-37\n",
      "  Processing bug: Lang-38\n",
      "  Processing bug: Lang-39\n",
      "  Processing bug: Lang-40\n",
      "    Error processing ./defects4j_data/Lang_40/src/java/org/apache/commons/lang/text/translate/EntityArrays.java: 'utf-8' codec can't decode byte 0xa9 in position 1702: invalid start byte\n",
      "  Processing bug: Lang-41\n",
      "    Error processing ./defects4j_data/Lang_41/src/java/org/apache/commons/lang/text/translate/EntityArrays.java: 'utf-8' codec can't decode byte 0xa9 in position 1702: invalid start byte\n",
      "  Processing bug: Lang-42\n",
      "  Processing bug: Lang-43\n",
      "  Processing bug: Lang-44\n",
      "  Processing bug: Lang-45\n",
      "  Processing bug: Lang-46\n",
      "  Processing bug: Lang-47\n",
      "  Processing bug: Lang-49\n",
      "  Processing bug: Lang-50\n",
      "  Processing bug: Lang-51\n",
      "  Processing bug: Lang-52\n",
      "  Processing bug: Lang-53\n",
      "  Processing bug: Lang-54\n",
      "  Processing bug: Lang-55\n",
      "  Processing bug: Lang-56\n",
      "  Processing bug: Lang-57\n",
      "  Processing bug: Lang-58\n",
      "  Processing bug: Lang-59\n",
      "  Processing bug: Lang-60\n",
      "  Processing bug: Lang-61\n",
      "  Processing bug: Lang-62\n",
      "  Processing bug: Lang-63\n",
      "  Processing bug: Lang-64\n",
      "  Processing bug: Lang-65\n",
      "Processing project: Math\n",
      "  Processing bug: Math-1\n",
      "  Processing bug: Math-2\n",
      "  Processing bug: Math-3\n",
      "  Processing bug: Math-4\n",
      "  Processing bug: Math-5\n",
      "  Processing bug: Math-6\n",
      "  Processing bug: Math-7\n",
      "  Processing bug: Math-8\n",
      "  Processing bug: Math-9\n",
      "  Processing bug: Math-10\n",
      "  Processing bug: Math-11\n",
      "  Processing bug: Math-12\n",
      "  Processing bug: Math-13\n",
      "  Processing bug: Math-14\n",
      "  Processing bug: Math-15\n",
      "  Processing bug: Math-16\n",
      "  Processing bug: Math-17\n",
      "  Processing bug: Math-18\n",
      "  Processing bug: Math-19\n",
      "  Processing bug: Math-20\n",
      "  Processing bug: Math-21\n",
      "  Processing bug: Math-22\n",
      "  Processing bug: Math-23\n",
      "  Processing bug: Math-24\n",
      "  Processing bug: Math-25\n",
      "  Processing bug: Math-26\n",
      "  Processing bug: Math-27\n",
      "  Processing bug: Math-28\n",
      "  Processing bug: Math-29\n",
      "  Processing bug: Math-30\n",
      "  Processing bug: Math-31\n",
      "  Processing bug: Math-32\n",
      "  Processing bug: Math-33\n",
      "  Processing bug: Math-34\n",
      "  Processing bug: Math-35\n",
      "  Processing bug: Math-36\n",
      "  Processing bug: Math-37\n",
      "  Processing bug: Math-38\n",
      "  Processing bug: Math-39\n",
      "  Processing bug: Math-40\n",
      "  Processing bug: Math-41\n",
      "  Processing bug: Math-42\n",
      "  Processing bug: Math-43\n",
      "  Processing bug: Math-44\n",
      "  Processing bug: Math-45\n",
      "  Processing bug: Math-46\n",
      "  Processing bug: Math-47\n",
      "  Processing bug: Math-48\n",
      "  Processing bug: Math-49\n",
      "  Processing bug: Math-50\n",
      "  Processing bug: Math-51\n",
      "  Processing bug: Math-52\n",
      "  Processing bug: Math-53\n",
      "  Processing bug: Math-54\n",
      "  Processing bug: Math-55\n",
      "  Processing bug: Math-56\n",
      "  Processing bug: Math-57\n",
      "  Processing bug: Math-58\n",
      "  Processing bug: Math-59\n",
      "  Processing bug: Math-60\n",
      "  Processing bug: Math-61\n",
      "  Processing bug: Math-62\n",
      "  Processing bug: Math-63\n",
      "  Processing bug: Math-64\n",
      "  Processing bug: Math-65\n",
      "  Processing bug: Math-66\n",
      "  Processing bug: Math-67\n",
      "  Processing bug: Math-68\n",
      "  Processing bug: Math-69\n",
      "  Processing bug: Math-70\n",
      "  Processing bug: Math-71\n",
      "  Processing bug: Math-72\n",
      "  Processing bug: Math-73\n",
      "  Processing bug: Math-74\n",
      "  Processing bug: Math-75\n",
      "  Processing bug: Math-76\n",
      "  Processing bug: Math-77\n",
      "  Processing bug: Math-78\n",
      "  Processing bug: Math-79\n",
      "  Processing bug: Math-80\n",
      "  Processing bug: Math-81\n",
      "  Processing bug: Math-82\n",
      "  Processing bug: Math-83\n",
      "  Processing bug: Math-84\n",
      "  Processing bug: Math-85\n",
      "  Processing bug: Math-86\n",
      "  Processing bug: Math-87\n",
      "  Processing bug: Math-88\n",
      "  Processing bug: Math-89\n",
      "  Processing bug: Math-90\n",
      "  Processing bug: Math-91\n",
      "  Processing bug: Math-92\n",
      "  Processing bug: Math-93\n",
      "  Processing bug: Math-94\n",
      "  Processing bug: Math-95\n",
      "  Processing bug: Math-96\n",
      "  Processing bug: Math-97\n",
      "  Processing bug: Math-98\n",
      "  Processing bug: Math-99\n",
      "  Processing bug: Math-100\n",
      "  Processing bug: Math-101\n",
      "  Processing bug: Math-102\n",
      "  Processing bug: Math-103\n",
      "  Processing bug: Math-104\n",
      "  Processing bug: Math-105\n",
      "  Processing bug: Math-106\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 530us/step - accuracy: 0.9954 - loss: 0.0353 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0116 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 479us/step - accuracy: 0.9983 - loss: 0.0112 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0114 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step - accuracy: 0.9985 - loss: 0.0107 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0114 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - accuracy: 0.9982 - loss: 0.0112 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0125 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 5/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step - accuracy: 0.9982 - loss: 0.0125 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0108 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 6/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474us/step - accuracy: 0.9983 - loss: 0.0097 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0106 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 7/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - accuracy: 0.9984 - loss: 0.0097 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0108 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 8/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step - accuracy: 0.9984 - loss: 0.0094 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0106 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 9/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step - accuracy: 0.9982 - loss: 0.0109 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0112 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 10/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step - accuracy: 0.9983 - loss: 0.0100 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0110 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 11/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step - accuracy: 0.9983 - loss: 0.0088 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0109 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 12/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472us/step - accuracy: 0.9985 - loss: 0.0086 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0122 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 13/50\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - accuracy: 0.9984 - loss: 0.0086 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9983 - val_loss: 0.0109 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m662/662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211us/step\n",
      "Confusion Matrix:\n",
      "[[21129     0]\n",
      " [   35     0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     21129\n",
      "           1       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           1.00     21164\n",
      "   macro avg       0.50      0.50      0.50     21164\n",
      "weighted avg       1.00      1.00      1.00     21164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import subprocess\n",
    "import re\n",
    "import javalang\n",
    "from collections import Counter\n",
    "\n",
    "class Defects4JBugDetector:\n",
    "    def __init__(self, defects4j_path, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the bug detector with paths to Defects4J and output directory\n",
    "        \n",
    "        Args:\n",
    "            defects4j_path: Path to Defects4J installation\n",
    "            output_dir: Directory to save extracted data and models\n",
    "        \"\"\"\n",
    "        self.defects4j_path = defects4j_path\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    def extract_features_from_java(self, java_code):\n",
    "        \"\"\"\n",
    "        Extract code features from Java source code\n",
    "        \n",
    "        Args:\n",
    "            java_code: String containing Java source code\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of code features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        features['loc'] = len(java_code.splitlines())\n",
    "        features['chars'] = len(java_code)\n",
    "        \n",
    "        # Count tokens and keywords\n",
    "        try:\n",
    "            tokens = list(javalang.tokenizer.tokenize(java_code))\n",
    "            token_types = Counter(token.__class__.__name__ for token in tokens)\n",
    "            \n",
    "            for token_type, count in token_types.items():\n",
    "                features[f'token_{token_type}'] = count\n",
    "                \n",
    "            # Try to parse and extract AST-based features\n",
    "            try:\n",
    "                tree = javalang.parse.parse(java_code)\n",
    "                \n",
    "                # Count different node types in AST\n",
    "                node_types = []\n",
    "                for path, node in tree:\n",
    "                    node_types.append(node.__class__.__name__)\n",
    "                \n",
    "                node_counts = Counter(node_types)\n",
    "                for node_type, count in node_counts.items():\n",
    "                    features[f'ast_{node_type}'] = count\n",
    "                    \n",
    "                # Extract cyclomatic complexity approximation\n",
    "                features['if_statements'] = node_counts.get('IfStatement', 0)\n",
    "                features['for_loops'] = node_counts.get('ForStatement', 0)\n",
    "                features['while_loops'] = node_counts.get('WhileStatement', 0)\n",
    "                features['try_blocks'] = node_counts.get('TryStatement', 0)\n",
    "                features['catch_blocks'] = node_counts.get('CatchClause', 0)\n",
    "                features['complexity'] = (1 + features['if_statements'] + \n",
    "                                         features['for_loops'] + \n",
    "                                         features['while_loops'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If AST parsing fails, set these features to 0\n",
    "                features['if_statements'] = 0\n",
    "                features['for_loops'] = 0\n",
    "                features['while_loops'] = 0\n",
    "                features['try_blocks'] = 0\n",
    "                features['catch_blocks'] = 0\n",
    "                features['complexity'] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            # If tokenization fails, set token features to 0\n",
    "            features['token_Identifier'] = 0\n",
    "            features['token_Keyword'] = 0\n",
    "            features['token_Operator'] = 0\n",
    "        \n",
    "        # Check for common bug indicators\n",
    "        features['null_checks'] = java_code.count('null')\n",
    "        features['todo_comments'] = len(re.findall(r'TODO|FIXME', java_code))\n",
    "        features['exception_handling'] = java_code.count('catch')\n",
    "        features['magic_numbers'] = len(re.findall(r'[^a-zA-Z0-9_\"\\']\\d+[^a-zA-Z0-9_\"\\'.]', java_code))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "\n",
    "    def extract_defects4j_data(self, projects=None):\n",
    "        \"\"\"\n",
    "        Extract data from Defects4J projects\n",
    "\n",
    "        Args:\n",
    "            projects: List of project names to process (None for all projects)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with code features and bug labels\n",
    "        \"\"\"\n",
    "        if projects is None:\n",
    "            # Get list of all projects\n",
    "            cmd = [os.path.join(self.defects4j_path, \"framework/bin/defects4j\"), \"pids\"]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            projects = result.stdout.strip().split()\n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        for project in projects:\n",
    "            print(f\"Processing project: {project}\")\n",
    "\n",
    "            # Get number of bugs in project\n",
    "            cmd = [os.path.join(self.defects4j_path, \"framework/bin/defects4j\"), \"bids\", \"-p\", project]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            bug_ids = result.stdout.strip().split()\n",
    "\n",
    "            for bug_id in bug_ids:\n",
    "                if not bug_id.strip():\n",
    "                    continue\n",
    "\n",
    "                print(f\"  Processing bug: {project}-{bug_id}\")\n",
    "                work_dir = os.path.join(self.output_dir, f\"{project}_{bug_id}\")\n",
    "                os.makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "                # Checkout buggy version\n",
    "                checkout_cmd = [\n",
    "                    os.path.join(self.defects4j_path, \"framework/bin/defects4j\"),\n",
    "                    \"checkout\", \n",
    "                    \"-p\", project, \n",
    "                    \"-v\", f\"{bug_id}b\", \n",
    "                    \"-w\", work_dir\n",
    "                ]\n",
    "                result = subprocess.run(checkout_cmd, capture_output=True, text=True)\n",
    "                if result.returncode != 0:\n",
    "                    print(f\"    Checkout failed for {project}-{bug_id}: {result.stderr.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # Get list of modified classes\n",
    "                export_cmd = [\n",
    "                    os.path.join(self.defects4j_path, \"framework/bin/defects4j\"), \n",
    "                    \"export\", \n",
    "                    \"-p\", \"classes.modified\", \n",
    "                    \"-w\", work_dir\n",
    "                ]\n",
    "                result = subprocess.run(export_cmd, capture_output=True, text=True)\n",
    "                buggy_classes = set(result.stdout.strip().split())\n",
    "\n",
    "                # Gather all Java files in the source directory\n",
    "                src_dir = os.path.join(work_dir, \"src\")\n",
    "                for root, _, files in os.walk(src_dir):\n",
    "                    for file in files:\n",
    "                        if not file.endswith(\".java\"):\n",
    "                            continue\n",
    "\n",
    "                        java_file = os.path.join(root, file)\n",
    "                        try:\n",
    "                            with open(java_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                                code = f.read()\n",
    "\n",
    "                            # Compute class name (Defects4J-style)\n",
    "                            rel_path = os.path.relpath(java_file, src_dir)\n",
    "                            class_name = rel_path.replace(os.sep, '.').replace(\".java\", \"\")\n",
    "\n",
    "                            # Determine if this file is buggy\n",
    "                            is_buggy = any(class_name.endswith(buggy_class) for buggy_class in buggy_classes)\n",
    "\n",
    "                            # Extract features\n",
    "                            features = self.extract_features_from_java(code)\n",
    "                            features.update({\n",
    "                                'project': project,\n",
    "                                'bug_id': bug_id,\n",
    "                                'file': class_name,\n",
    "                                'is_buggy': int(is_buggy)\n",
    "                            })\n",
    "\n",
    "                            all_data.append(features)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error processing {java_file}: {e}\")\n",
    "\n",
    "                # Clean up\n",
    "                subprocess.run([\"rm\", \"-rf\", work_dir])\n",
    "\n",
    "        if all_data:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            df.to_csv(os.path.join(self.output_dir, \"defects4j_features.csv\"), index=False)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data was collected.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    \n",
    "    def prepare_data(self, df=None, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Prepare data for training\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with features (if None, loads from disk)\n",
    "            test_size: Proportion of data to use for testing\n",
    "            \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test, scaler\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            df = pd.read_csv(f\"{self.output_dir}/defects4j_features.csv\")\n",
    "        \n",
    "        # Select features (drop non-feature columns)\n",
    "        non_features = ['project', 'bug_id', 'file', 'is_buggy']\n",
    "        X = df.drop(columns=[col for col in non_features if col in df.columns])\n",
    "        y = df['is_buggy']\n",
    "        \n",
    "        # Fill missing values\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # Store the feature column names for later use in prediction\n",
    "        self.feature_columns = list(X.columns)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Save the feature columns to use in prediction\n",
    "        if not hasattr(scaler, 'feature_names_in_'):\n",
    "            # For older sklearn versions that don't store feature names\n",
    "            setattr(scaler, 'feature_names_in_', self.feature_columns)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "    \n",
    "    def build_model(self, input_dim):\n",
    "        \"\"\"\n",
    "        Build neural network model\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            \n",
    "        Returns:\n",
    "            Compiled Keras model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_dim=input_dim),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the neural network model\n",
    "        \n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_test, y_test: Test data\n",
    "            epochs: Number of epochs\n",
    "            batch_size: Batch size\n",
    "            \n",
    "        Returns:\n",
    "            Trained model and history\n",
    "        \"\"\"\n",
    "        # Build model\n",
    "        model = self.build_model(X_train.shape[1])\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        model.save(f\"{self.output_dir}/bug_detector_model.h5\")\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            X_test, y_test: Test data\n",
    "            \n",
    "        Returns:\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        # Predict\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        return conf_matrix, report\n",
    "\n",
    "def predict_bugs(self, model, scaler, java_file_path):\n",
    "    \"\"\"\n",
    "    Predict if a Java file contains bugs\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        scaler: Feature scaler\n",
    "        java_file_path: Path to Java file\n",
    "        \n",
    "    Returns:\n",
    "        Bug probability\n",
    "    \"\"\"\n",
    "    # Read file\n",
    "    with open(java_file_path, 'r', encoding='utf-8') as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    # Extract features\n",
    "    features = self.extract_features_from_java(code)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([features])\n",
    "    \n",
    "    # Get the original feature names used during training\n",
    "    # Try to load them from scaler or from a saved instance variable\n",
    "    if hasattr(scaler, 'feature_names_in_'):\n",
    "        original_features = list(scaler.feature_names_in_)\n",
    "    else:\n",
    "        # If not available, this might be a problem\n",
    "        print(\"Warning: Scaler doesn't have feature_names_in_ attribute.\")\n",
    "        if hasattr(self, 'feature_columns'):\n",
    "            original_features = self.feature_columns\n",
    "        else:\n",
    "            raise ValueError(\"Cannot determine original feature names. Store them during training.\")\n",
    "    \n",
    "    # Create a new DataFrame with all expected columns from training\n",
    "    aligned_df = pd.DataFrame(0.0, index=[0], columns=original_features)\n",
    "    \n",
    "    # Fill in values for features that exist in our current data\n",
    "    for col in df.columns:\n",
    "        if col in aligned_df.columns:\n",
    "            aligned_df[col] = df[col]\n",
    "    \n",
    "    # Make sure all values are numeric and replace NAs\n",
    "    aligned_df = aligned_df.fillna(0).astype(float)\n",
    "    \n",
    "    # Make sure columns are in the exact same order as during training\n",
    "    aligned_df = aligned_df[original_features]\n",
    "    \n",
    "    # Now scale the features\n",
    "    X = scaler.transform(aligned_df)\n",
    "    \n",
    "    # Predict\n",
    "    bug_prob = model.predict(X)[0][0]\n",
    "    \n",
    "    return bug_prob \n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector\n",
    "    detector = Defects4JBugDetector(\n",
    "        defects4j_path=\"/Users/clairecallon/defects4j\",\n",
    "        output_dir=\"./defects4j_data\"\n",
    "    )\n",
    "    \n",
    "    # Extract data (comment out if already done)\n",
    "    df = detector.extract_defects4j_data(['Lang', 'Math'])\n",
    "    \n",
    "    # Load existing data\n",
    "    df = pd.read_csv(\"./defects4j_data/defects4j_features.csv\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test, scaler = detector.prepare_data()\n",
    "    \n",
    "    # Train model\n",
    "    model, history = detector.train_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    detector.evaluate_model(model, X_test, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- missing_feature_0\n- missing_feature_1\n- missing_feature_10\n- missing_feature_11\n- missing_feature_12\n- ...\nFeature names seen at fit time, yet now missing:\n- ast_AnnotationDeclaration\n- ast_AnnotationMethod\n- ast_ArrayCreator\n- ast_ArrayInitializer\n- ast_ArraySelector\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bug_prob\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Predict on a new file\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_bugs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/Math_35_b/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBug probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 360\u001b[0m, in \u001b[0;36mDefects4JBugDetector.predict_bugs\u001b[0;34m(self, model, scaler, java_file_path)\u001b[0m\n\u001b[1;32m    357\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, :feature_names]\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m    363\u001b[0m bug_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/utils/validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[1;32m   2836\u001b[0m     _estimator,\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m   2844\u001b[0m ):\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[1;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[1;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bugloc/lib/python3.10/site-packages/sklearn/utils/validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- missing_feature_0\n- missing_feature_1\n- missing_feature_10\n- missing_feature_11\n- missing_feature_12\n- ...\nFeature names seen at fit time, yet now missing:\n- ast_AnnotationDeclaration\n- ast_AnnotationMethod\n- ast_ArrayCreator\n- ast_ArrayInitializer\n- ast_ArraySelector\n- ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict on a new file\n",
    "prob = detector.predict_bugs(model, scaler, \"/tmp/Math_35_b/src/test/java/org/apache/commons/math3/geometry/euclidean/twod/LineTest.java\")\n",
    "print(f\"Bug probability: {prob:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bugloc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
